import nltk
from nltk.tokenize import TweetTokenizer, sent_tokenize
from coroutine import *

if nltk.download('genesis') & nltk.download('punkt'): 
    print('Genesis downloaded successfully')

# Extract some corpus in english
# Extract some corpus in other languages
# Label each word
# Put all the word in array
# Randomize it
# 70 - 30 (maybe delete the anseen words in 70 and duplicates)
# Write the model
# Apply the model to the dataset
# Create the confusion matrix



#sent_text = sent_tokenize(nltk.corpus.genesis) # this gives us a list of sentences
# now loop over each sentence and tokenize it separately









print('Hello')